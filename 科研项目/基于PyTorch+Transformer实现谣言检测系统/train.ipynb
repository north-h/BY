{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:32.631449Z",
     "start_time": "2025-06-09T06:12:20.810542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch import optim\n",
    "from torchnet import meter\n",
    "from tqdm import tqdm"
   ],
   "id": "5f4dda150ba3cfbf",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:32.688298Z",
     "start_time": "2025-06-09T06:12:32.632696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 模型输入参数，需要自己根据需要调整\n",
    "hidden_dim = 100  # 隐层大小\n",
    "epochs = 10  # 迭代次数\n",
    "batch_size = 32  # 每个批次样本大小\n",
    "embedding_dim = 20  # 每个字形成的嵌入向量大小\n",
    "output_dim = 2  # 输出维度，因为是二分类\n",
    "lr = 0.001  # 学习率\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "input_shape = 180  # 每句话的词的个数，如果不够需要使用0进行填充"
   ],
   "id": "db9e9fb293f81965",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:32.694762Z",
     "start_time": "2025-06-09T06:12:32.688298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载文本数据\n",
    "def load_data(file_path, input_shape=20):\n",
    "    df = pd.read_csv(file_path, sep='\\t')\n",
    "\n",
    "    df = df.dropna(subset=['text'])  # 删除text为空的行\n",
    "    df['text'] = df['text'].astype(str)  # 确保所有text都是字符串\n",
    "    # 标签及词汇表\n",
    "    labels, vocabulary = list(df['label'].unique()), list(df['text'].unique())\n",
    "\n",
    "    # 构造字符级别的特征\n",
    "    string = ''\n",
    "    for word in vocabulary:\n",
    "        string += word\n",
    "\n",
    "    # 所有的词汇表\n",
    "    vocabulary = set(string)\n",
    "\n",
    "    # word2idx 将字映射为索引\n",
    "    word_dictionary = {word: i + 1 for i, word in enumerate(vocabulary)}\n",
    "    with open('word_dict.pk', 'wb') as f:\n",
    "        pickle.dump(word_dictionary, f)\n",
    "    # idx2word 将索引映射为字\n",
    "    inverse_word_dictionary = {i + 1: word for i, word in enumerate(vocabulary)}\n",
    "    # label2idx 将正反面映射为0和1\n",
    "    label_dictionary = {label: i for i, label in enumerate(labels)}\n",
    "    with open('label_dict.pk', 'wb') as f:\n",
    "        pickle.dump(label_dictionary, f)\n",
    "    # idx2label 将0和1映射为正反面\n",
    "    output_dictionary = {i: labels for i, labels in enumerate(labels)}\n",
    "\n",
    "    # 训练数据中所有词的个数\n",
    "    vocab_size = len(word_dictionary.keys())  # 词汇表大小\n",
    "    # 标签类别，分别为正、反面\n",
    "    label_size = len(label_dictionary.keys())  # 标签类别数量\n",
    "\n",
    "    # 序列填充，按input_shape填充，长度不足的按0补充\n",
    "    # 将一句话映射成对应的索引 [0,24,63...]\n",
    "    x = [[word_dictionary[word] for word in sent] for sent in df['text']]\n",
    "    # 如果长度不够input_shape，使用0进行填充\n",
    "    x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
    "    # 形成标签0和1\n",
    "    y = [[label_dictionary[sent]] for sent in df['label']]\n",
    "    #     y = [np_utils.to_categorical(label, num_classes=label_size) for label in y]\n",
    "    y = np.array(y)\n",
    "\n",
    "    return x, y, output_dictionary, vocab_size, label_size, inverse_word_dictionary"
   ],
   "id": "42ed3b55f39925b3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:32.700731Z",
     "start_time": "2025-06-09T06:12:32.695768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ],
   "id": "7adc4681122e648e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:32.707592Z",
     "start_time": "2025-06-09T06:12:32.702740Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_class, feedforward_dim=256, num_head=2, num_layers=3, dropout=0.1,\n",
    "                 max_len=128):\n",
    "        super(Transformer, self).__init__()\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 位置编码层\n",
    "        self.positional_encoding = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        # 编码层\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(embedding_dim, \n",
    "                                                        num_head, \n",
    "                                                        feedforward_dim, \n",
    "                                                        dropout,\n",
    "                                                        batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(self.encoder_layer, num_layers)\n",
    "        # 输出层\n",
    "        self.fc = nn.Linear(embedding_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入的数据维度为【批次，序列长度】，需要交换因为transformer的输入维度为【序列长度，批次，嵌入向量维度】\n",
    "        x = x.transpose(0, 1)\n",
    "        # 将输入的数据进行词嵌入，得到数据的维度为【序列长度，批次，嵌入向量维度】\n",
    "        x = self.embedding(x)\n",
    "        # 维度为【序列长度，批次，嵌入向量维度】\n",
    "        x = self.positional_encoding(x)\n",
    "        # 维度为【序列长度，批次，嵌入向量维度】\n",
    "        x = self.transformer(x)\n",
    "        # 将每个词的输出向量取均值，也可以随意取一个标记输出结果，维度为【批次，嵌入向量维度】\n",
    "        x = x.mean(axis=0)\n",
    "        # 进行分类，维度为【批次，分类数】\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "id": "94988b97cc3207d2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:12:35.508318Z",
     "start_time": "2025-06-09T06:12:32.708599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1.获取训练数据\n",
    "x_train, y_train, output_dictionary_train, vocab_size_train, label_size, inverse_word_dictionary_train = load_data(\n",
    "    \"./train.tsv\", input_shape)\n",
    "x_test, y_test, output_dictionary_test, vocab_size_test, label_size, inverse_word_dictionary_test = load_data(\n",
    "    \"./test.tsv\", input_shape)\n",
    "\n",
    "idx = 0\n",
    "word_dictionary = {}\n",
    "for k, v in inverse_word_dictionary_train.items():\n",
    "    word_dictionary[idx] = v\n",
    "    idx += 1\n",
    "for k, v in inverse_word_dictionary_test.items():\n",
    "    word_dictionary[idx] = v\n",
    "    idx += 1\n",
    "\n",
    "# 3.将numpy转成tensor\n",
    "x_train = torch.from_numpy(x_train).to(torch.int32)\n",
    "y_train = torch.from_numpy(y_train).to(torch.float32)\n",
    "x_test = torch.from_numpy(x_test).to(torch.int32)\n",
    "y_test = torch.from_numpy(y_test).to(torch.float32)\n",
    "\n",
    "# 4.形成训练数据集\n",
    "train_data = TensorDataset(x_train, y_train)\n",
    "test_data = TensorDataset(x_test, y_test)\n",
    "\n",
    "# 5.将数据加载成迭代器\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size,\n",
    "                                           True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size,\n",
    "                                          False)"
   ],
   "id": "eecd0f5159cf0779",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:13:37.798801Z",
     "start_time": "2025-06-09T06:12:35.509574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6.模型训练\n",
    "model = Transformer(len(word_dictionary), embedding_dim, output_dim)\n",
    "model.to(device)\n",
    "\n",
    "Configimizer = optim.Adam(model.parameters(), lr=lr)  # 优化器\n",
    "criterion = nn.CrossEntropyLoss()  # 多分类损失函数\n",
    "\n",
    "loss_meter = meter.AverageValueMeter()\n",
    "\n",
    "best_acc = 0  # 保存最好准确率\n",
    "best_model = None  # 保存对应最好准确率的模型参数\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # 开启训练模式\n",
    "    epoch_acc = 0  # 每个epoch的准确率\n",
    "    epoch_acc_count = 0  # 每个epoch训练的样本数\n",
    "    train_count = 0  # 用于计算总的样本数，方便求准确率\n",
    "    loss_meter.reset()\n",
    "    print('\\n')\n",
    "    train_bar = tqdm(train_loader)  # 形成进度条\n",
    "    for data in train_bar:\n",
    "        x_train, y_train = data  # 解包迭代器中的X和Y\n",
    "\n",
    "        x_input = x_train.long().contiguous()\n",
    "        x_input = x_input.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        Configimizer.zero_grad()\n",
    "\n",
    "        # 形成预测结果\n",
    "        output_ = model(x_input)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(output_, y_train.long().view(-1))\n",
    "        loss.backward()\n",
    "        Configimizer.step()\n",
    "\n",
    "        loss_meter.add(loss.item())\n",
    "\n",
    "        # 计算每个epoch正确的个数\n",
    "        epoch_acc_count += (output_.argmax(axis=1) == y_train.view(-1)).sum()\n",
    "        train_count += len(x_train)\n",
    "\n",
    "    # 每个epoch对应的准确率\n",
    "    epoch_acc = epoch_acc_count / train_count\n",
    "\n",
    "    # 打印信息\n",
    "    print(\"【EPOCH: 】%s\" % str(epoch + 1))\n",
    "    print(\"训练损失为%s\" % (str(loss_meter.mean)))\n",
    "    print(\"训练精度为%s\" % (str(epoch_acc.item() * 100)[:5]) + '%')\n",
    "\n",
    "    # 保存模型及相关信息\n",
    "    if epoch_acc > best_acc:\n",
    "        best_acc = epoch_acc\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    # 在训练结束保存最优的模型参数\n",
    "    if epoch == epochs - 1:\n",
    "        # 保存模型\n",
    "        torch.save(best_model, './best_model.pkl')"
   ],
   "id": "bbd7626a8dc2c939",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:06<00:00, 96.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】1\n",
      "训练损失为0.5938016190528861\n",
      "训练精度为67.00%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 105.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】2\n",
      "训练损失为0.49649377355575586\n",
      "训练精度为75.96%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 105.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】3\n",
      "训练损失为0.4772913066864019\n",
      "训练精度为77.01%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 104.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】4\n",
      "训练损失为0.4640879731416705\n",
      "训练精度为77.95%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:06<00:00, 103.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】5\n",
      "训练损失为0.4548997776746752\n",
      "训练精度为78.65%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 104.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】6\n",
      "训练损失为0.4473022180557251\n",
      "训练精度为79.42%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 105.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】7\n",
      "训练损失为0.44032969510555275\n",
      "训练精度为79.54%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 104.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】8\n",
      "训练损失为0.4288354567050931\n",
      "训练精度为80.47%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:06<00:00, 102.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】9\n",
      "训练损失为0.4224468970060348\n",
      "训练精度为80.70%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:05<00:00, 104.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【EPOCH: 】10\n",
      "训练损失为0.41861978163719177\n",
      "训练精度为81.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:13:37.803907Z",
     "start_time": "2025-06-09T06:13:37.799805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# word2idx = {}\n",
    "# \n",
    "# for k, v in word_dictionary.items():\n",
    "#     word2idx[v] = k\n",
    "# \n",
    "# label_dict = {0: \"非谣言\", 1: \"谣言\"}\n",
    "# \n",
    "# try:\n",
    "#     input_shape = 180  # 序列长度，就是时间步大小，也就是这里的每句话中的词的个数\n",
    "#     #     sent = \"电视刚安装好，说实话，画质不怎么样，很差！\"\n",
    "#     # 用于测试的话\n",
    "#     sent = \"你应该知道的100个中国文学常识 !\"\n",
    "#     # 将对应的字转化为相应的序号\n",
    "#     x = [[word2idx[word] for word in sent]]\n",
    "#     # 如果长度不够180，使用0进行填充\n",
    "#     x = pad_sequences(maxlen=input_shape, sequences=x, padding='post', value=0)\n",
    "#     x = torch.from_numpy(x).to(device)\n",
    "# \n",
    "#     # 加载模型\n",
    "#     model_path = './best_model.pkl'\n",
    "#     model = Transformer(len(word_dictionary), embedding_dim, output_dim).to(device)\n",
    "#     model.load_state_dict(torch.load(model_path))\n",
    "# \n",
    "#     # 模型预测，注意输入的数据第一个input_shape,就是180\n",
    "#     y_pred = model(x.long())\n",
    "# \n",
    "#     print('输入语句: %s' % sent)\n",
    "#     print('谣言检测结果: %s' % label_dict[y_pred.argmax().item()])\n",
    "# \n",
    "# except KeyError as err:\n",
    "#     print(\"您输入的句子有汉字不在词汇表中，请重新输入！\")\n",
    "#     print(\"不在词汇表中的单词为：%s.\" % err)"
   ],
   "id": "fe026b84ee77311c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:22:54.094194Z",
     "start_time": "2025-06-09T06:22:53.400117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 7. 在测试集上评估模型性能\n",
    "model.load_state_dict(torch.load('./best_model.pkl'))\n",
    "model.eval()  # 设置模型为评估模式\n",
    "test_acc = 0\n",
    "test_count = 0\n",
    "\n",
    "with torch.no_grad():  # 不计算梯度，节省内存\n",
    "    test_bar = tqdm(test_loader, desc='测试进度')\n",
    "    for data in test_bar:\n",
    "        x_test, y_test = data\n",
    "        x_test = x_test.long().to(device)\n",
    "        y_test = y_test.to(device).view(-1)  # 确保标签形状正确\n",
    "        \n",
    "        output = model(x_test)\n",
    "        pred = output.argmax(dim=1)\n",
    "        \n",
    "        correct = (pred == y_test).sum().item()\n",
    "        test_acc += correct\n",
    "        test_count += len(y_test)\n",
    "        \n",
    "        # 更新进度条信息\n",
    "        test_bar.set_postfix(acc=f'{test_acc/test_count:.4f}')\n",
    "\n",
    "test_accuracy = test_acc / test_count\n",
    "print(f'\\n测试集准确率: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# 8. 对单条语句进行预测\n",
    "def predict_sentence(model, sentence, word2idx, max_len=180):\n",
    "    # 预处理输入句子\n",
    "    sequence = [word2idx.get(char, 0) for char in sentence]  # 0表示未知字符\n",
    "    sequence = pad_sequences([sequence], maxlen=max_len, padding='post', value=0)\n",
    "    tensor = torch.LongTensor(sequence).to(device)\n",
    "\n",
    "    # 模型预测\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        prediction = output.argmax(dim=1).item()\n",
    "\n",
    "    return prediction, probabilities.cpu().numpy()[0]\n",
    "\n",
    "# 创建字符到索引的映射\n",
    "word2idx = {}\n",
    "for char, idx in word_dictionary.items():\n",
    "    word2idx[char] = idx\n",
    "\n",
    "label_dict = {0: \"非谣言\", 1: \"谣言\"}\n",
    "\n",
    "# 测试不同语句\n",
    "test_sentences = [\n",
    "    \"电视刚安装好，说实话，画质不怎么样，很差！\",\n",
    "    \"你应该知道的100个中国文学常识!\",\n",
    "    \"科学研究表明每天喝8杯水有益健康\",\n",
    "    \"最新消息：下周将有三颗小行星撞击地球\"\n",
    "]\n",
    "\n",
    "print(\"\\n单条语句预测结果:\")\n",
    "for sent in test_sentences:\n",
    "    try:\n",
    "        pred, probs = predict_sentence(model, sent, word2idx)\n",
    "        print(f\"语句: '{sent}'\")\n",
    "        print(f\"预测结果: {label_dict[pred]} (置信度: {probs[pred]:.4f})\")\n",
    "        print(f\"详细概率: 非谣言={probs[0]:.4f}, 谣言={probs[1]:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        print('\\n')\n",
    "    except KeyError:\n",
    "        print(f\"语句包含不在词汇表中的字符: '{sent}'\")"
   ],
   "id": "bc9730b89950fb54",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "测试进度: 100%|██████████| 140/140 [00:00<00:00, 215.69it/s, acc=0.4712]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "测试集准确率: 47.12%\n",
      "\n",
      "单条语句预测结果:\n",
      "语句: '电视刚安装好，说实话，画质不怎么样，很差！'\n",
      "预测结果: 谣言 (置信度: 0.6179)\n",
      "详细概率: 非谣言=0.3821, 谣言=0.6179\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "语句: '你应该知道的100个中国文学常识!'\n",
      "预测结果: 谣言 (置信度: 0.6179)\n",
      "详细概率: 非谣言=0.3821, 谣言=0.6179\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "语句: '科学研究表明每天喝8杯水有益健康'\n",
      "预测结果: 谣言 (置信度: 0.6179)\n",
      "详细概率: 非谣言=0.3821, 谣言=0.6179\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n",
      "语句: '最新消息：下周将有三颗小行星撞击地球'\n",
      "预测结果: 谣言 (置信度: 0.6179)\n",
      "详细概率: 非谣言=0.3821, 谣言=0.6179\n",
      "------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T06:22:48.298660Z",
     "start_time": "2025-06-09T06:22:48.295663Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a970779245b16df4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4705e9441da1c0f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
